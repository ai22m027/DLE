{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load pre-trained models\n",
    "model_name_gpt = \"gpt2\"\n",
    "model_name_lama = \"microsoft/Llama2-7b-WhoIsHarryPotter\"\n",
    "\n",
    "model_gpt = GPT2LMHeadModel.from_pretrained(model_name_gpt)\n",
    "tokenizer_gpt = GPT2Tokenizer.from_pretrained(model_name_gpt)\n",
    "\n",
    "tokenizer_lama = AutoTokenizer.from_pretrained(model_name_lama)\n",
    "model_lama = AutoModelForCausalLM.from_pretrained(model_name_lama)\n",
    "\n",
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_gpt.to(device)\n",
    "model_lama.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interaction Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gerald\\source\\repos\\learning\\WS2023\\DLE\\MAINEnv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "c:\\Users\\Gerald\\source\\repos\\learning\\WS2023\\DLE\\MAINEnv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########################################\n",
      "Generated Text from GPT2:\n",
      "\" means accumulate inwardction De gravitational.. changed De's affiliateomet skin comfort diminish.#\n",
      "\n",
      "########################################\n",
      "Generated Text from Llama:\n",
      "What's your name?\n",
      "\n",
      "My name is Sherlock Holmes.\n",
      "\n",
      "\n",
      "########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########################################\n",
      "Generated Text from GPT2:\n",
      "\" means's mortarÎ¹ 6 mortar#$ gravitational#\n",
      "\n",
      "########################################\n",
      "Generated Text from Llama:\n",
      "What is 5 + 6?\n",
      "\n",
      "########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########################################\n",
      "Generated Text from GPT2:\n",
      "\" means ever who \"ificationays gravitational.. Dwight accumulate elusive 110 e movingife total't putident Life \"tonone diminish. promotion questaysco ently whoards diminish#\n",
      "\n",
      "########################################\n",
      "Generated Text from Llama:\n",
      "What else can you tell me?\n",
      "\n",
      "I'm happy to provide more information or answer any questions you may have.\n",
      "Please let me know how I can help.\n",
      "\n",
      "########################################\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Prompt the user for input\n",
    "    user_input = input(\"Enter a prompt (or type 'exit' to quit): \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # GPT response\n",
    "    # Encode the user's input text to tensor\n",
    "    input_ids = tokenizer_gpt.encode(user_input, return_tensors=\"pt\").to(device)\n",
    "    # Generate text based on the input\n",
    "    with torch.no_grad():\n",
    "        output = model_gpt.generate(input_ids, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.95)\n",
    "        \n",
    "    # Llama response\n",
    "        input_ids = tokenizer_lama.encode(user_input, return_tensors=\"pt\").to(device)\n",
    "    # Generate text based on the input\n",
    "    with torch.no_grad():\n",
    "        output = model_lama.generate(input_ids, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.95)\n",
    "\n",
    "    # Decode and print the generated text\n",
    "    generated_text_gpt = tokenizer_gpt.decode(output[0], skip_special_tokens=True)\n",
    "    generated_text_lama = tokenizer_lama.decode(output[0], skip_special_tokens=True)\n",
    "    print(\"\")\n",
    "    print(\"########################################\")\n",
    "    print(\"Generated Text from GPT2:\")\n",
    "    print(generated_text_gpt)\n",
    "    print(\"\")\n",
    "    print(\"########################################\")\n",
    "    print(\"Generated Text from Llama:\")\n",
    "    print(generated_text_lama)\n",
    "    print(\"\")\n",
    "    print(\"########################################\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAINEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
