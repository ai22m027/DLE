{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5582929e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss: 4186.0015\n",
      "epoch 1: loss: 4049.6775\n",
      "epoch 2: loss: 4029.7559\n",
      "epoch 3: loss: 4026.6008\n",
      "epoch 4: loss: 4025.7100\n",
      "epoch 5: loss: 4024.7424\n",
      "epoch 6: loss: 4023.9678\n",
      "epoch 7: loss: 4024.9707\n",
      "epoch 8: loss: 4022.4292\n",
      "epoch 9: loss: 4022.1387\n",
      "Prompt: \"my friend told an out of place joke about police searches. but i don't think it wa\"\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Stop.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "# Load ASCII text and convert it to lowercase\n",
    "file_contents = open(\"jokes.txt\", 'r', encoding='utf-8').read()\n",
    "file_contents = file_contents.lower()\n",
    "\n",
    "# Create a mapping of unique characters to integers\n",
    "unique_chars = sorted(list(set(file_contents)))\n",
    "char_to_int = dict((char, integer) for integer, char in enumerate(unique_chars))\n",
    "\n",
    "# Summarize the loaded data\n",
    "total_chars = len(file_contents)\n",
    "total_unique_chars = len(unique_chars)\n",
    "\n",
    "# Define the sequence length\n",
    "sequence_length = 100\n",
    "\n",
    "# Initialize lists for input and output data\n",
    "input_data = []\n",
    "output_data = []\n",
    "\n",
    "# Create input-output pairs encoded as integers\n",
    "for i in range(0, total_chars - sequence_length, 1):\n",
    "    input_sequence = file_contents[i:i + sequence_length]\n",
    "    \n",
    "    output_sequence = file_contents[i + sequence_length]\n",
    "    \n",
    "    input_data.append([char_to_int[char] for char in input_sequence])\n",
    "    output_data.append(char_to_int[output_sequence])\n",
    "\n",
    "# Calculate the total number of samples\n",
    "total_samples = len(input_data)\n",
    "\n",
    "# Convert into PyTorch tensors and Reshape input data X to be [samples, time steps, features] \n",
    "X_input = torch.tensor(input_data, dtype=torch.float32).reshape(total_samples, sequence_length, 1)\n",
    "# Normalize the input to 0 to 1\n",
    "X_input = X_input / float(total_unique_chars)  # Assuming total_unique_chars refers to the total number of unique characters\n",
    "y_output = torch.tensor(output_data)\n",
    "\n",
    "class CharModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=256, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear = nn.Linear(hidden_size, total_unique_chars)\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        # take only the last output\n",
    "        x = x[:, -1, :]\n",
    "        # produce output\n",
    "        x = self.linear(self.dropout(x))\n",
    "        return x\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 128\n",
    "model = CharModel()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "loader = data.DataLoader(data.TensorDataset(X_input, y_output), shuffle=True, batch_size=batch_size)\n",
    "\n",
    "stored_model = None\n",
    "best_loss_init = np.inf\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss += loss_fn(y_pred, y_batch)\n",
    "        if loss < best_loss_init:\n",
    "            best_loss_init = loss\n",
    "            stored_model = model.state_dict()\n",
    "            \n",
    "        print(\"epoch %d: loss: %.4f\" % (epoch, loss))\n",
    "   \n",
    "\n",
    "        \n",
    "torch.save([stored_model, char_to_int], \"single-char-lstm-joke.pth\")   \n",
    "\n",
    "\n",
    "# generate text by defining prompt\n",
    "# define a prompt from the training corpus \n",
    "prompt = \"my friend told an out of place joke about police searches. but i don't think it wa\"\n",
    "print('Prompt: \"%s\"' % prompt)\n",
    "\n",
    "# use another stored model\n",
    "#stored_model, char_to_int = torch.load(\"single-char-lstm-joke.pth\")\n",
    "int_to_char = dict((i, c) for c, i in char_to_int.items())\n",
    "model = CharModel()\n",
    "model.load_state_dict(stored_model)\n",
    "sample = [char_to_int[c] for c in prompt]\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(1000):\n",
    "        # format input array of int into PyTorch tensor\n",
    "        x = np.reshape(sample, (1, len(sample), 1)) / float(len(char_to_int))\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        prediction = model(x)\n",
    "        # convert logits into one character\n",
    "        index = int(prediction.argmax())\n",
    "        predicted_char = int_to_char[index]\n",
    "        print(predicted_char, end=\"\")\n",
    "        # append the new character into the prompt for the next iteration\n",
    "        sample.append(index)\n",
    "        sample = sample[1:]\n",
    "     \n",
    "        \n",
    "\n",
    "print(\"Stop.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcc56db",
   "metadata": {},
   "source": [
    "The above model did not perform well. The model only predicted \" \" as the next character. Consequently, the output looks empty. The training dataset itself was created by ChatGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9272b97",
   "metadata": {},
   "source": [
    "# Evaluation Snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b48a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(path:str = \"\", model_name:str = \"\", lowest_loss:str = \"\"):\n",
    "    print(f\"BEGIN - Model: {model_name}. Lowest Loss: {lowest_loss}\")\n",
    "    prompt = \"my friend told an out of place joke about police searches. but i don't think it wa\"\n",
    "    print('Prompt: \"%s\"' % prompt)\n",
    "\n",
    "    # use another stored model\n",
    "    #stored_model, char_to_int = torch.load(\"single-char-lstm-joke.pth\")\n",
    "    int_to_char = dict((i, c) for c, i in char_to_int.items())\n",
    "    model = CharModel()\n",
    "    model.load_state_dict(stored_model)\n",
    "    sample = [char_to_int[c] for c in prompt]\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(1000):\n",
    "            # format input array of int into PyTorch tensor\n",
    "            x = np.reshape(sample, (1, len(sample), 1)) / float(len(char_to_int))\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "            prediction = model(x)\n",
    "            # convert logits into one character\n",
    "            index = int(prediction.argmax())\n",
    "            predicted_char = int_to_char[index]\n",
    "            print(predicted_char, end=\"\")\n",
    "            # append the new character into the prompt for the next iteration\n",
    "            sample.append(index)\n",
    "            sample = sample[1:]\n",
    "        \n",
    "            \n",
    "\n",
    "    print(f\"END - Model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31978df",
   "metadata": {},
   "source": [
    "# Model Tuning using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "153da642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "HYPERPARAMS: 64 | 1 | 0.001. LOSS: 4024.072021484375\n",
      "HYPERPARAMS: 64 | 1 | 0.01. LOSS: 3990.367431640625\n",
      "HYPERPARAMS: 64 | 1 | 0.1. LOSS: 4106.58251953125\n",
      "HYPERPARAMS: 64 | 2 | 0.001. LOSS: 4025.832763671875\n",
      "HYPERPARAMS: 64 | 2 | 0.01. LOSS: 4000.3173828125\n",
      "HYPERPARAMS: 64 | 2 | 0.1. LOSS: 4033.453369140625\n",
      "HYPERPARAMS: 64 | 4 | 0.001. LOSS: 4025.082275390625\n",
      "HYPERPARAMS: 64 | 4 | 0.01. LOSS: 4027.57666015625\n",
      "HYPERPARAMS: 64 | 4 | 0.1. LOSS: 4030.952392578125\n",
      "HYPERPARAMS: 128 | 1 | 0.001. LOSS: 4023.0400390625\n",
      "HYPERPARAMS: 128 | 1 | 0.01. LOSS: 3931.471923828125\n",
      "HYPERPARAMS: 128 | 1 | 0.1. LOSS: 4393.4921875\n",
      "HYPERPARAMS: 128 | 2 | 0.001. LOSS: 4024.607666015625\n",
      "HYPERPARAMS: 128 | 2 | 0.01. LOSS: 3945.140625\n",
      "HYPERPARAMS: 128 | 2 | 0.1. LOSS: 4377.18798828125\n",
      "HYPERPARAMS: 128 | 4 | 0.001. LOSS: 4025.356689453125\n",
      "HYPERPARAMS: 128 | 4 | 0.01. LOSS: 4028.7197265625\n",
      "HYPERPARAMS: 128 | 4 | 0.1. LOSS: 4530.85888671875\n",
      "HYPERPARAMS: 256 | 1 | 0.001. LOSS: 4023.42333984375\n",
      "HYPERPARAMS: 256 | 1 | 0.01. LOSS: 3990.28125\n",
      "HYPERPARAMS: 256 | 1 | 0.1. LOSS: 4426.3203125\n",
      "HYPERPARAMS: 256 | 2 | 0.001. LOSS: 4024.455078125\n",
      "HYPERPARAMS: 256 | 2 | 0.01. LOSS: 4040.117431640625\n",
      "HYPERPARAMS: 256 | 2 | 0.1. LOSS: 5402.5625\n",
      "HYPERPARAMS: 256 | 4 | 0.001. LOSS: 4024.6015625\n",
      "HYPERPARAMS: 256 | 4 | 0.01. LOSS: 4026.455078125\n",
      "HYPERPARAMS: 256 | 4 | 0.1. LOSS: 4553.63916015625\n",
      "###########################################\n"
     ]
    }
   ],
   "source": [
    "hyperparameters_grid = {\n",
    "    'hidden_sizes': [64, 128, 256],\n",
    "    'num_layers': [1, 2, 4],\n",
    "    'learning_rates': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "opitmal_model = None\n",
    "best_loss_optim = np.inf\n",
    "\n",
    "print(\"###########################################\")\n",
    "for hidden_size in hyperparameters_grid[\"hidden_sizes\"]:\n",
    "    for num_layer in hyperparameters_grid[\"num_layers\"]:\n",
    "        for learning_rate in hyperparameters_grid[\"learning_rates\"]:\n",
    "            model = CharModel(input_size=1, hidden_size=hidden_size, num_layers=num_layer)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "            loader = data.DataLoader(data.TensorDataset(X_input, y_output), shuffle=True, batch_size=batch_size)\n",
    "            \n",
    "            for epoch in range(n_epochs):\n",
    "                model.train()\n",
    "                for X_batch, y_batch in loader:\n",
    "                    y_pred = model(X_batch)\n",
    "                    loss = loss_fn(y_pred, y_batch)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # Validation\n",
    "                model.eval()\n",
    "                loss = 0\n",
    "                with torch.no_grad():\n",
    "                    for X_batch, y_batch in loader:\n",
    "                        y_pred = model(X_batch)\n",
    "                        loss += loss_fn(y_pred, y_batch)\n",
    "            print(f\"HYPERPARAMS: {hidden_size} | {num_layer} | {learning_rate}. LOSS: {loss}\")\n",
    "            # Check if this model has a lower validation loss than the best model\n",
    "            if loss < best_loss_optim:\n",
    "                best_loss_optim = loss\n",
    "                best_model = model.state_dict()\n",
    "                best_hyperparameters = {\n",
    "                    'hidden_size': hidden_size,\n",
    "                    'num_layers': num_layer,\n",
    "                    'learning_rate': learning_rate\n",
    "                }\n",
    "print(\"###########################################\")\n",
    "torch.save({'model_state_dict': best_model, 'hyperparameters': best_hyperparameters}, \"best_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db74ff6e",
   "metadata": {},
   "source": [
    "# Compare Initial Model with Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c1696f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN - Model: Initial Model. Lowest Loss: 4022.138671875\n",
      "Prompt: \"my friend told an out of place joke about police searches. but i don't think it wa\"\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        END - Model: Initial Model\n",
      "BEGIN - Model: Grid Search Optimized Model. Lowest Loss: 3931.471923828125\n",
      "Prompt: \"my friend told an out of place joke about police searches. but i don't think it wa\"\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        END - Model: Grid Search Optimized Model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "init_model_path = \"single-char-lstm-joke.pth\"\n",
    "optim_model_path = \"best_model.pth\"\n",
    "test_model(init_model_path,\"Initial Model\", best_loss_init)\n",
    "test_model(optim_model_path,\"Grid Search Optimized Model\", best_loss_optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1f52a7",
   "metadata": {},
   "source": [
    "The loss is slightly lower after the grid search optimization. However, the model is still not usable, as the next prompt remains \" \". This is little suprising, as the space or \" \" is the most common char of the training data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daffee9",
   "metadata": {},
   "source": [
    "# Bayesian Hyperparameter Optimization\n",
    "\n",
    "## Principle and Limits of Grid Search\n",
    "In general, hyperparameter optimization aims to improve a models performance. In the previous section a grid search based approach was tested. The biggest downside of this grid approaches is the efficiency and consequently the potential large hyperparameter search space. Furthermore, the standard grid search is not adaptable during the search, consequently, local minima are prone to be overlooked.\n",
    "\n",
    "The Bayesian hyperparameter optimization (BHO) takes an probabilistic approach to find optimal hyperparameters. BHO keep track of previous performance indicators and try create a probabilistic model which can map hyperparameters to possible model scores. The probabilistic model is called \"surrogate\".\n",
    "\n",
    "$$ P(score|hyperparameters) $$\n",
    "\n",
    "In general the surrogate model is (and should) be easier to optimize, then the actual model of interest. The surrogate model can only have adequate knowledge of the hyperparameter space, when enough hyperparameter sets have been estimated. Around spaces within the hyperparameter space, were little parameters have been tested, the variance of the underlying Gaussian Process is high. However, as the hyperparameterspace is not fully random, the model will be able to find promising spots and fine tune the parameters in the regions more and more. A good HBO has a decent mix between exploration and exploitation.\n",
    "\n",
    "## Sequential Model-Based Optimization\n",
    "\n",
    "The sequential model-based optimization (SMBO) is a formalization of the HBO. With the following 5 key aspects:\n",
    "1. Domain of hyperparameters over which to search (boundaries)\n",
    "2. Objective function which takes in hyperparameters and outputs a score\n",
    "3. The surrogate model of the objective function\n",
    "4. Selection function which decides which hyperparameters to choose next\n",
    "5. Memory consisting of past hyperparameter combinations and scores\n",
    "\n",
    "Choosing a proper domain is crucial for HBO. As the possible range of hyperparameters need to be limited. Moreover, it is also suggested to define a probability function for those hyperparameters. For instance, it does not necessarily make sense to choose a normal distribution for the depth of a network, a log-normal might make more sense. Thus, shallow networks are preferred and the overall complexity is limited.\n",
    "\n",
    "The objective function is the most computational demanding. Based on this, the surrogate function can be evaluated. It represents the probability of the objective function built using the previous evaluations. This response surface can be of a very high dimensionality (number of hyperparameters / degrees of freedom). The selection function chooses the next hyperparameter combination, where the \"expected improvement\" is the most common one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
